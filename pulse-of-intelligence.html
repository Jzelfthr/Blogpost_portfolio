<!DOCTYPE html>
criterion = nn.CrossEntropyLoss()
losses = []
for epoch in range(5):
running = 0.0
for images, labels in train_loader:
optimizer.zero_grad()
outputs = model(images)
loss = criterion(outputs, labels)
loss.backward()
optimizer.step()
running += loss.item()
losses.append(running / len(train_loader))
results[lr] = losses
</code></pre>
</div>


<h3>Results (visualize)</h3>
<p>Below is a placeholder for the loss comparison plot. Replace with `assets/plots/lr-loss-compare.png` generated from your run.</p>


<div class="plot-placeholder">
<img src="../assets/plots/lr-loss-placeholder.png" alt="loss plot placeholder" style="max-width:100%" />
</div>


</section>


<!-- 3. Analyze your findings -->
<section id="analyze">
<h2>3. Analyze Findings</h2>


<h3>Observations</h3>
<ul>
<li><strong>0.0001:</strong> Very stable but slow; underfits in short training.</li>
<li><strong>0.001:</strong> Balanced; stable convergence and good accuracy.</li>
<li><strong>0.01:</strong> Too large; loss oscillates or diverges.</li>
</ul>


<h3>Did it improve performance or stability?</h3>
<p>
The mid-range learning rate improved both speed and stability. Too large a learning rate harmed stability and final accuracy.
</p>


<h3>Insights for future tuning</h3>
<ol>
<li>Start with sensible defaults (Adam: 0.001). Use LR finder tools when possible.</li>
<li>Apply LR schedules (StepLR, CosineAnnealing, ReduceLROnPlateau) for improved training.</li>
<li>Combine with warmup and appropriate batch size for large-scale training.</li>
</ol>


</section>


<section id="references">
<h2>References</h2>
<ul>
<li>Goodfellow, Bengio, Courville — Deep Learning</li>
<li>PyTorch Documentation</li>
<li>CS231n — Stanford</li>
</ul>
</section>


<footer class="post-footer">
<p>© 2025 Jezzel Faith Gier</p>
</footer>


</article>
</div>


<script src="../js/main.js"></script>
</body>
</html>
